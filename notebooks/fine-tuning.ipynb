{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f07365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from GPTModel import GPTModel\n",
    "from Generate import generate\n",
    "from EncodeDecode import text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a705b827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                                msg\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/SMSSpamCollection.tsv',sep = '\\t') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5da60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible choices in column 'type':\n",
      "['ham' 'spam']\n",
      "Number of 'ham': 4825\n",
      "Number of 'spam': 747\n"
     ]
    }
   ],
   "source": [
    "valeurs_uniques = df['type'].unique()\n",
    "print(\"Possible choices in column 'type':\")\n",
    "print(valeurs_uniques)\n",
    "\n",
    "print(\"Number of 'ham':\", df[df['type'] == 'ham'].shape[0])\n",
    "print(\"Number of 'spam':\", df[df['type'] == 'spam'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96e4cd",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766c26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ham = df[df['type'] == 'ham']\n",
    "spam = df[df['type'] == 'spam']\n",
    "\n",
    "ham_undersampled = ham.sample(n=len(spam), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([ham_undersampled, spam])\n",
    "\n",
    "print(df_balanced['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400370f",
   "metadata": {},
   "source": [
    "### Create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Calculate split indices\n",
    "n = len(df_balanced)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = train_end + int(0.1 * n)\n",
    "\n",
    "# Split\n",
    "train = df_balanced.iloc[:train_end]\n",
    "val = df_balanced.iloc[train_end:val_end]\n",
    "test = df_balanced.iloc[val_end:]\n",
    "\n",
    "train.to_csv('../data/fine-tuning/train.csv', index=False)\n",
    "val.to_csv('../data/fine-tuning/validation.csv', index=False)\n",
    "test.to_csv('../data/fine-tuning/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8151fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_length, tokenizer, pad_token_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing the dataset.\n",
    "            max_length (int or None): Maximum length of the tokenized vector.\n",
    "                If None, all vectors are resized to the longest vector in the dataset.\n",
    "            tokenizer: The tokenizer used to encode the strings.\n",
    "            pad_token_id (int): The token ID used for padding.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.input_ids = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Tokenize all texts and determine the max length if not provided\n",
    "        self._tokenize_and_preprocess()\n",
    "\n",
    "    def _tokenize_and_preprocess(self):\n",
    "        # Tokenize all texts\n",
    "        texts = self.data['msg'].tolist()\n",
    "        labels = self.data['type'].tolist()\n",
    "\n",
    "        # Tokenize each text\n",
    "        tokenized_texts = [self.tokenizer.encode(text, return_tensors='pt').squeeze(0) for text in texts]\n",
    "\n",
    "        # Determine the max length if not provided\n",
    "        if self.max_length is None:\n",
    "            self.max_length = max(len(t) for t in tokenized_texts)\n",
    "\n",
    "        # Pad or truncate each sequence\n",
    "        for tokens in tokenized_texts:\n",
    "            if len(tokens) < self.max_length:\n",
    "                # Pad with pad_token_id\n",
    "                padded_tokens = torch.cat([\n",
    "                    tokens,\n",
    "                    torch.full((self.max_length - len(tokens),), self.pad_token_id)\n",
    "                ])\n",
    "                self.input_ids.append(padded_tokens)\n",
    "            else:\n",
    "                # Truncate\n",
    "                self.input_ids.append(tokens[:self.max_length])\n",
    "\n",
    "        # Convert labels to numerical values (e.g., 'ham' -> 0, 'spam' -> 1)\n",
    "        label_map = {'ham': 0, 'spam': 1}\n",
    "        self.labels = [label_map[label] for label in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            self.input_ids[idx], torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac0cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "trainDataSet = SpamDataset('../data/fine-tuning/train.csv', None, tokenizer, eos_token_id)\n",
    "valDataSet = SpamDataset('../data/fine-tuning/validation.csv', 256, tokenizer, eos_token_id)\n",
    "testDataSet = SpamDataset('../data/fine-tuning/test.csv', 256, tokenizer, eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d920",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5125af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1045\n",
      "Number of validation samples: 149\n",
      "Number of test samples: 300\n"
     ]
    }
   ],
   "source": [
    "trainDataLoader = DataLoader(trainDataSet, batch_size=8, shuffle=True, drop_last=True, num_workers=0)\n",
    "valDataLoader = DataLoader(valDataSet, batch_size=8, shuffle=False, drop_last=False, num_workers=0)\n",
    "testDataLoader = DataLoader(testDataSet, batch_size=8, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "print(\"Number of training samples:\", len(trainDataSet))\n",
    "print(\"Number of validation samples:\", len(valDataSet))\n",
    "print(\"Number of test samples:\", len(testDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25cd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(target_tensor: torch.Tensor, source_array) -> torch.Tensor:\n",
    "    source_tensor = torch.tensor(source_array, dtype=target_tensor.dtype)\n",
    "\n",
    "    if target_tensor.shape != source_tensor.shape:\n",
    "        raise ValueError(\n",
    "            f\"Incompatible shapes: target {tuple(target_tensor.shape)} \"\n",
    "            f\"vs source {tuple(source_tensor.shape)}\"\n",
    "        )\n",
    "\n",
    "    return source_tensor\n",
    "\n",
    "def load_weights_into_gpt(model, params):\n",
    "    emb_dim = model.emb_dim\n",
    "    n_heads = model.n_heads\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # --- Embeddings ---\n",
    "        model.token_embedding.weight.copy_(\n",
    "            assign(model.token_embedding.weight, params[\"wte\"])\n",
    "        )\n",
    "        model.pos_embedding_layer.weight.copy_(\n",
    "            assign(model.pos_embedding_layer.weight, params[\"wpe\"])\n",
    "        )\n",
    "        \n",
    "        # --- Blocs Transformer ---\n",
    "        for i, block in enumerate(model.transformer_blocks):\n",
    "            p_block = params[\"blocks\"][i]\n",
    "            \n",
    "            # LayerNorm 1\n",
    "            block.layerNorm1.scale.copy_(\n",
    "                assign(block.layerNorm1.scale, p_block[\"ln_1\"][\"g\"])\n",
    "            )\n",
    "            block.layerNorm1.shift.copy_(\n",
    "                assign(block.layerNorm1.shift, p_block[\"ln_1\"][\"b\"])\n",
    "            )\n",
    "            \n",
    "            # --- ATTENTION : Gestion Correcte de c_attn ---\n",
    "            W_qkv = p_block[\"attn\"][\"c_attn\"][\"w\"]  # [768, 2304]\n",
    "            b_qkv = p_block[\"attn\"][\"c_attn\"][\"b\"]  # [2304]\n",
    "            \n",
    "            # Découpage sur la dimension de sortie (colonnes)\n",
    "            W_q = W_qkv[:, 0*emb_dim:1*emb_dim]  # [768, 768]\n",
    "            W_k = W_qkv[:, 1*emb_dim:2*emb_dim]\n",
    "            W_v = W_qkv[:, 2*emb_dim:3*emb_dim]\n",
    "            \n",
    "            b_q = b_qkv[0*emb_dim:1*emb_dim]  # [768]\n",
    "            b_k = b_qkv[1*emb_dim:2*emb_dim]\n",
    "            b_v = b_qkv[2*emb_dim:3*emb_dim]\n",
    "            \n",
    "            # Vérification de shape (CRITIQUE)\n",
    "            assert W_q.shape == (emb_dim, emb_dim), f\"❌ W_q shape: {W_q.shape}\"\n",
    "            assert b_q.shape == (emb_dim,), f\"❌ b_q shape: {b_q.shape}\"\n",
    "            \n",
    "            # Copie dans les Linear (bias=False donc on ignore les biais)\n",
    "            # PyTorch: Linear.weight est (out_features, in_features)\n",
    "            block.mha.W_query.weight.copy_(assign(block.mha.W_query.weight, W_q.T))\n",
    "            block.mha.W_query.bias.copy_(assign(block.mha.W_query.bias, b_q))\n",
    "\n",
    "            block.mha.W_key.weight.copy_(assign(block.mha.W_key.weight, W_k.T))\n",
    "            block.mha.W_key.bias.copy_(assign(block.mha.W_key.bias, b_k))\n",
    "\n",
    "            block.mha.W_value.weight.copy_(assign(block.mha.W_value.weight, W_v.T))\n",
    "            block.mha.W_value.bias.copy_(assign(block.mha.W_value.bias, b_v))\n",
    "            \n",
    "            # Projection de sortie\n",
    "            W_o = p_block[\"attn\"][\"c_proj\"][\"w\"]  # [768, 768]\n",
    "            b_o = p_block[\"attn\"][\"c_proj\"][\"b\"]\n",
    "            block.mha.out_proj.weight.copy_(assign(block.mha.out_proj.weight, W_o.T))\n",
    "            block.mha.out_proj.bias.copy_(assign(block.mha.out_proj.bias, b_o))\n",
    "            \n",
    "            # LayerNorm 2\n",
    "            block.layerNorm2.scale.copy_(\n",
    "                assign(block.layerNorm2.scale, p_block[\"ln_2\"][\"g\"])\n",
    "            )\n",
    "            block.layerNorm2.shift.copy_(\n",
    "                assign(block.layerNorm2.shift, p_block[\"ln_2\"][\"b\"])\n",
    "            )\n",
    "            \n",
    "            # --- MLP ---\n",
    "            W1 = p_block[\"mlp\"][\"c_fc\"][\"w\"]  # [768, 3072]\n",
    "            b1 = p_block[\"mlp\"][\"c_fc\"][\"b\"]  # [3072]\n",
    "            \n",
    "            assert W1.shape == (emb_dim, 4*emb_dim), f\"❌ W1 shape: {W1.shape}\"\n",
    "            \n",
    "            block.feedForward.layer1.weight.copy_(assign(block.feedForward.layer1.weight, W1.T))\n",
    "            block.feedForward.layer1.bias.copy_(assign(block.feedForward.layer1.bias, b1))\n",
    "            \n",
    "            W2 = p_block[\"mlp\"][\"c_proj\"][\"w\"]  # [3072, 768]\n",
    "            b2 = p_block[\"mlp\"][\"c_proj\"][\"b\"]  # [768]\n",
    "            \n",
    "            block.feedForward.layer2.weight.copy_(assign(block.feedForward.layer2.weight, W2.T))\n",
    "            block.feedForward.layer2.bias.copy_(assign(block.feedForward.layer2.bias, b2))\n",
    "        \n",
    "        # LayerNorm final (stored at root level as 'g' and 'b')\n",
    "        model.layerNorm.scale.copy_(\n",
    "            assign(model.layerNorm.scale, params[\"g\"])\n",
    "        )\n",
    "        model.layerNorm.shift.copy_(\n",
    "            assign(model.layerNorm.shift, params[\"b\"])\n",
    "        )\n",
    "        \n",
    "        # Weight tying (inverseEmbedding == token_embedding)\n",
    "        if hasattr(model, \"inverseEmbedding\"):\n",
    "            model.inverseEmbedding.weight.copy_(\n",
    "                assign(model.inverseEmbedding.weight, params[\"wte\"])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2609fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ec03f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../model/gpt\\124M\\checkpoint\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\encoder.json\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\hparams.json\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: ../model/gpt\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(\"124M\", \"../model/gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.0,\n",
    "    'qkv_bias': True,\n",
    "    \"out_bias\": True\n",
    "}\n",
    "\n",
    "model = GPTModel(config)\n",
    "model.to(device)\n",
    "load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6db17",
   "metadata": {},
   "source": [
    "### Can the base GPT2 model perform the classification using only a prompt? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddeed47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "Click on any of the questions below to go directly to your winning submission. The links below\n"
     ]
    }
   ],
   "source": [
    "input = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "batch = text_to_token_ids(input)\n",
    "batch = batch.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_tokens = generate(model, batch, max_new_tokens=20, context_size=1024, temperature=1, top_k=30)\n",
    "\n",
    "generated_text = token_ids_to_text(generated_tokens)\n",
    "\n",
    "print(input + generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
